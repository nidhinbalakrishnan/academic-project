academic-project
================

Proximity based Sentiment Analysis with Contextual Phrase Polarity

Sentiment Analysis is used to identify and extract subjective information in source material in source materials with the use of Natural Language Processing and various other techniques. In this project, we are using a new methodology for Sentiment Analysis called Proximity based Sentiment Analysis. In addition to this, we are also considering Phrase Polarity in order to improve the accuracy of the result. Also we are trying to implement a Dynamic dictionary to optimise the dictionary size and to improve the access time.

---------------------------------------------------------------------------------------------------------

Tools and Techniques used:

Classification Techniques used :- Median Approach, Proximity Distribution, Machine Learning Approach.

Tools & Languages used :- Python 2.7, NLTK 3.0, SentiWordNet 3.0.0, scikit-learn, Stanford Parser, numpy.

Phrase polarity will be incorporated in the second phase to improve the accuracy of the result.

Instead of a Static Dictionary, a Dynamic Dictionary will be implemented in the final phase to optimise the Dictionary size and improve the access time.

---------------------------------------------------------------------------------------------------------

Procedure for Installing and setting up the tools:
	Inorder to train the data, we are using around 700 positive and 700 negative movie reviews. We are using the reviews which are available in the nltk corpus. So if you need to train the system, you need to install nltk first. The link containing instructions for installing nltk on any machine is given below
		http://www.nltk.org/install.html

	After installing nltk, you need to download the nltk_data which contains the nltk corpus containing many data including 1000 positive and negative movie reviews each. Inorder to download the nltk_data, we need to use the NLTK's data downloader. Instructions for downloading the nltk_data is available in the following link (nltk_data consists of many corpora and it is of about 1.8GB in size)
		http://www.nltk.org/data.html

	You also needs to install various packages such as scikit-learn, scipy and numpy. Site containing Unofficial Windows Binaries of these packages is given below
		http://www.lfd.uci.edu/~gohlke/pythonlibs/

	Downlod the packages and Install. For Linux and Mac, you can follow the instructions on the following links
		http://scikit-learn.org/stable/install.html
		http://penandpants.com/2012/02/24/install-python/
		http://www.scipy.org/install.html 

---------------------------------------------------------------------------------------------------------











